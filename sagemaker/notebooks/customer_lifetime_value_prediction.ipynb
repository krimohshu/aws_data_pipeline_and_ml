{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0116203",
   "metadata": {},
   "source": [
    "# Customer Lifetime Value (CLV) Prediction\n",
    "\n",
    "## üìö  Overview\n",
    "\n",
    "In this notebook, we'll build a **Machine Learning model** to predict customer lifetime value using the customer segments we created with EMR.\n",
    "\n",
    "### What We'll Learn:\n",
    "1. **Exploratory Data Analysis (EDA)** - Understanding our data\n",
    "2. **Feature Engineering** - Creating predictive features\n",
    "3. **Model Training** - Random Forest Regression\n",
    "4. **Model Evaluation** - Metrics and validation\n",
    "5. **Predictions** - Making business decisions\n",
    "\n",
    "### Business Problem:\n",
    "**Goal**: Predict how much revenue each customer will generate in the next 12 months\n",
    "\n",
    "**Why it matters**:\n",
    "- Budget allocation (how much to spend on retention vs acquisition)\n",
    "- Identify high-value customers for VIP treatment\n",
    "- Predict churn risk (low predicted CLV = potential churner)\n",
    "- Personalize marketing spend per customer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac22e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "print(f\"   Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c654d0",
   "metadata": {},
   "source": [
    "## 1. Load Data from S3\n",
    "\n",
    "We'll load the customer segments data that EMR created using K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ace25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 configuration\n",
    "CURATED_BUCKET = 'data-lake-curated-zone-616129051451'\n",
    "PROCESSED_BUCKET = 'data-lake-processed-zone-616129051451'\n",
    "SCRIPTS_BUCKET = 'data-lake-scripts-616129051451'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "print(\"üìä Loading customer segments from S3...\")\n",
    "\n",
    "# Load all segments\n",
    "segments_dfs = []\n",
    "for segment_id in range(4):\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=CURATED_BUCKET,\n",
    "        Prefix=f'customer-segments/segment={segment_id}/'\n",
    "    )\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'].endswith('.parquet'):\n",
    "            # Download from S3\n",
    "            s3.download_file(CURATED_BUCKET, obj['Key'], f'/tmp/segment_{segment_id}.parquet')\n",
    "            df = pd.read_parquet(f'/tmp/segment_{segment_id}.parquet')\n",
    "            segments_dfs.append(df)\n",
    "\n",
    "# Combine all segments\n",
    "customers_df = pd.concat(segments_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(customers_df)} customers\")\n",
    "print(f\"   Columns: {list(customers_df.columns)}\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8b55a",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's understand our data before building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"   Shape: {customers_df.shape}\")\n",
    "print(f\"   Memory: {customers_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(\"\\nüìà Data Types:\")\n",
    "print(customers_df.dtypes)\n",
    "print(\"\\nüìâ Missing Values:\")\n",
    "print(customers_df.isnull().sum())\n",
    "print(\"\\nüìä Numerical Statistics:\")\n",
    "customers_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment distribution\n",
    "print(\"\\nüéØ Customer Segments Distribution:\")\n",
    "segment_dist = customers_df['segment_label'].value_counts()\n",
    "print(segment_dist)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Segment counts\n",
    "segment_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c', '#f39c12', '#3498db'])\n",
    "axes[0].set_title('Customer Count by Segment', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Segment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by segment\n",
    "revenue_by_segment = customers_df.groupby('segment_label')['total_spent'].sum().sort_values(ascending=False)\n",
    "revenue_by_segment.plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c', '#f39c12', '#3498db'])\n",
    "axes[1].set_title('Total Revenue by Segment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Segment')\n",
    "axes[1].set_ylabel('Total Spent ($)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí∞ Revenue by Segment:\")\n",
    "print(revenue_by_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7551148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"\\nüîó Feature Correlations:\")\n",
    "numeric_cols = ['total_spent', 'purchase_count', 'days_since_purchase', 'segment']\n",
    "correlation_matrix = customers_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74de688",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create new features that help predict customer lifetime value.\n",
    "\n",
    "### Features We'll Create:\n",
    "1. **avg_order_value** = total_spent / purchase_count\n",
    "2. **recency_score** = Inverse of days_since_purchase (recent = higher score)\n",
    "3. **frequency_score** = Normalized purchase count\n",
    "4. **monetary_score** = Normalized total spent\n",
    "5. **engagement_score** = Combined metric\n",
    "6. **One-hot encoding** for categorical features (region, payment method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_features = customers_df.copy()\n",
    "\n",
    "print(\"üîß Engineering features...\")\n",
    "\n",
    "# 1. Average order value\n",
    "df_features['avg_order_value'] = df_features['total_spent'] / df_features['purchase_count']\n",
    "\n",
    "# 2. Recency score (inverse of days since purchase, normalized)\n",
    "max_days = df_features['days_since_purchase'].max()\n",
    "df_features['recency_score'] = 1 - (df_features['days_since_purchase'] / max_days)\n",
    "\n",
    "# 3. Frequency score (normalized purchase count)\n",
    "max_purchases = df_features['purchase_count'].max()\n",
    "df_features['frequency_score'] = df_features['purchase_count'] / max_purchases\n",
    "\n",
    "# 4. Monetary score (normalized total spent)\n",
    "max_spent = df_features['total_spent'].max()\n",
    "df_features['monetary_score'] = df_features['total_spent'] / max_spent\n",
    "\n",
    "# 5. Engagement score (weighted combination)\n",
    "df_features['engagement_score'] = (\n",
    "    0.3 * df_features['recency_score'] +\n",
    "    0.3 * df_features['frequency_score'] +\n",
    "    0.4 * df_features['monetary_score']\n",
    ")\n",
    "\n",
    "# 6. Encode categorical features\n",
    "# Region\n",
    "region_dummies = pd.get_dummies(df_features['primary_region'], prefix='region')\n",
    "df_features = pd.concat([df_features, region_dummies], axis=1)\n",
    "\n",
    "# Payment method\n",
    "payment_dummies = pd.get_dummies(df_features['preferred_payment'], prefix='payment')\n",
    "df_features = pd.concat([df_features, payment_dummies], axis=1)\n",
    "\n",
    "# Segment label encoding\n",
    "le = LabelEncoder()\n",
    "df_features['segment_encoded'] = le.fit_transform(df_features['segment_label'])\n",
    "\n",
    "print(f\"‚úÖ Created {len(df_features.columns) - len(customers_df.columns)} new features\")\n",
    "print(f\"   Total features now: {len(df_features.columns)}\")\n",
    "print(\"\\nüìä New features sample:\")\n",
    "df_features[['customer_id', 'avg_order_value', 'recency_score', 'frequency_score', \n",
    "             'monetary_score', 'engagement_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1bd6a",
   "metadata": {},
   "source": [
    "## 4. Create Target Variable (CLV)\n",
    "\n",
    "### What is Customer Lifetime Value (CLV)?\n",
    "\n",
    "**CLV** = Predicted revenue a customer will generate over their lifetime (or a specific period).\n",
    "\n",
    "### Our Approach:\n",
    "Since we don't have future data, we'll create a **proxy CLV** based on historical behavior:\n",
    "\n",
    "```\n",
    "CLV_12_months = (total_spent / days_active) * 365\n",
    "```\n",
    "\n",
    "This estimates annual revenue based on their historical spending rate.\n",
    "\n",
    "**In production**, you'd use:\n",
    "- Actual 12-month revenue (if you have historical data)\n",
    "- Cohort analysis\n",
    "- Survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: 12-month CLV estimate\n",
    "# Assume customers have been active for 'days_since_purchase' days\n",
    "df_features['days_active'] = df_features['days_since_purchase'].clip(lower=1)  # Avoid division by zero\n",
    "df_features['clv_12_months'] = (df_features['total_spent'] / df_features['days_active']) * 365\n",
    "\n",
    "# Cap extreme values (prevent unrealistic predictions)\n",
    "df_features['clv_12_months'] = df_features['clv_12_months'].clip(upper=df_features['clv_12_months'].quantile(0.95))\n",
    "\n",
    "print(\"üéØ Target Variable Created: 12-Month CLV\")\n",
    "print(f\"   Mean CLV: ${df_features['clv_12_months'].mean():.2f}\")\n",
    "print(f\"   Median CLV: ${df_features['clv_12_months'].median():.2f}\")\n",
    "print(f\"   Min CLV: ${df_features['clv_12_months'].min():.2f}\")\n",
    "print(f\"   Max CLV: ${df_features['clv_12_months'].max():.2f}\")\n",
    "\n",
    "# Visualize CLV distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_features['clv_12_months'], bins=10, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('CLV Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('12-Month CLV ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot by segment\n",
    "df_features.boxplot(column='clv_12_months', by='segment_label', ax=axes[1])\n",
    "axes[1].set_title('CLV by Customer Segment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Segment')\n",
    "axes[1].set_ylabel('12-Month CLV ($)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176833e",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data\n",
    "\n",
    "Split data into:\n",
    "- **Training set** (80%) - Used to train the model\n",
    "- **Test set** (20%) - Used to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'total_spent',\n",
    "    'purchase_count',\n",
    "    'days_since_purchase',\n",
    "    'avg_order_value',\n",
    "    'recency_score',\n",
    "    'frequency_score',\n",
    "    'monetary_score',\n",
    "    'engagement_score',\n",
    "    'segment_encoded'\n",
    "]\n",
    "\n",
    "# Add one-hot encoded region columns\n",
    "region_cols = [col for col in df_features.columns if col.startswith('region_')]\n",
    "feature_cols.extend(region_cols)\n",
    "\n",
    "# Add one-hot encoded payment columns\n",
    "payment_cols = [col for col in df_features.columns if col.startswith('payment_')]\n",
    "feature_cols.extend(payment_cols)\n",
    "\n",
    "X = df_features[feature_cols]\n",
    "y = df_features['clv_12_months']\n",
    "\n",
    "print(f\"üìä Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"   Number of features: {X.shape[1]}\")\n",
    "print(f\"   Number of samples: {X.shape[0]}\")\n",
    "print(f\"\\nüéØ Target Variable Shape: {y.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data Split Complete:\")\n",
    "print(f\"   Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5490dc5",
   "metadata": {},
   "source": [
    "## 6. Train Random Forest Model\n",
    "\n",
    "### What is Random Forest?\n",
    "\n",
    "**Random Forest** is an ensemble learning algorithm that:\n",
    "1. Creates many decision trees (a \"forest\")\n",
    "2. Each tree makes a prediction\n",
    "3. Final prediction = average of all trees\n",
    "\n",
    "### Why Random Forest?\n",
    "- ‚úÖ Handles non-linear relationships\n",
    "- ‚úÖ Works well with small datasets\n",
    "- ‚úÖ Provides feature importance\n",
    "- ‚úÖ Resistant to overfitting\n",
    "- ‚úÖ No feature scaling required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest Regressor...\")\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=10,          # Maximum tree depth\n",
    "    min_samples_split=2,   # Minimum samples to split a node\n",
    "    min_samples_leaf=1,    # Minimum samples in leaf node\n",
    "    random_state=42,       # For reproducibility\n",
    "    n_jobs=-1              # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\")\n",
    "print(f\"   Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"   Number of features used: {rf_model.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ab0c2",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "### Metrics We'll Use:\n",
    "1. **R¬≤ Score** (0-1): How much variance the model explains (higher = better)\n",
    "2. **RMSE** (Root Mean Squared Error): Average prediction error in dollars\n",
    "3. **MAE** (Mean Absolute Error): Average absolute prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cad669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüéØ R¬≤ Score (Variance Explained):\")\n",
    "print(f\"   Training: {train_r2:.4f}\")\n",
    "print(f\"   Test:     {test_r2:.4f}\")\n",
    "print(f\"\\nüìâ RMSE (Root Mean Squared Error):\")\n",
    "print(f\"   Training: ${train_rmse:.2f}\")\n",
    "print(f\"   Test:     ${test_rmse:.2f}\")\n",
    "print(f\"\\nüìè MAE (Mean Absolute Error):\")\n",
    "print(f\"   Training: ${train_mae:.2f}\")\n",
    "print(f\"   Test:     ${test_mae:.2f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6, color='blue')\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_title(f'Training Set (R¬≤ = {train_r2:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Actual CLV ($)')\n",
    "axes[0].set_ylabel('Predicted CLV ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_title(f'Test Set (R¬≤ = {test_r2:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Actual CLV ($)')\n",
    "axes[1].set_ylabel('Predicted CLV ($)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f0e5e",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "Which features are most important for predicting CLV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üéØ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 10 Feature Importance for CLV Prediction', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dbfd1",
   "metadata": {},
   "source": [
    "## 9. Make Predictions on New Customers\n",
    "\n",
    "Let's predict CLV for all customers and add business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict CLV for all customers\n",
    "df_features['predicted_clv'] = rf_model.predict(X)\n",
    "\n",
    "# Add prediction confidence (based on proximity to training data)\n",
    "# Simple approach: compare to segment average\n",
    "segment_avg_clv = df_features.groupby('segment_label')['predicted_clv'].mean()\n",
    "df_features['segment_avg_clv'] = df_features['segment_label'].map(segment_avg_clv)\n",
    "df_features['clv_vs_segment_avg'] = df_features['predicted_clv'] / df_features['segment_avg_clv']\n",
    "\n",
    "# Business recommendations\n",
    "def get_recommendation(row):\n",
    "    clv = row['predicted_clv']\n",
    "    segment = row['segment_label']\n",
    "    \n",
    "    if clv > 500:\n",
    "        return \"üåü VIP Treatment - Personal account manager, exclusive offers\"\n",
    "    elif clv > 300:\n",
    "        return \"üíé Premium Care - Priority support, loyalty rewards\"\n",
    "    elif clv > 150:\n",
    "        return \"üìà Growth Potential - Upsell campaigns, engagement programs\"\n",
    "    elif segment == \"At-Risk\":\n",
    "        return \"‚ö†Ô∏è Retention Focus - Win-back campaign with 20% discount\"\n",
    "    else:\n",
    "        return \"üìä Standard Care - Regular newsletters, seasonal promotions\"\n",
    "\n",
    "df_features['recommendation'] = df_features.apply(get_recommendation, axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ CUSTOMER LIFETIME VALUE PREDICTIONS & RECOMMENDATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "results = df_features[[\n",
    "    'customer_id', 'segment_label', 'total_spent', 'purchase_count',\n",
    "    'predicted_clv', 'recommendation'\n",
    "]].sort_values('predicted_clv', ascending=False)\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä CLV SUMMARY BY SEGMENT:\")\n",
    "print(\"=\"*100)\n",
    "summary = df_features.groupby('segment_label').agg({\n",
    "    'customer_id': 'count',\n",
    "    'predicted_clv': ['mean', 'min', 'max', 'sum']\n",
    "}).round(2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df34311",
   "metadata": {},
   "source": [
    "## 10. Save Model to S3\n",
    "\n",
    "Save the trained model so we can use it later for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally first\n",
    "model_filename = '/tmp/clv_rf_model.joblib'\n",
    "joblib.dump(rf_model, model_filename)\n",
    "print(f\"‚úÖ Model saved locally: {model_filename}\")\n",
    "\n",
    "# Upload to S3\n",
    "model_s3_key = 'ml-models/customer_lifetime_value/clv_rf_model.joblib'\n",
    "s3.upload_file(model_filename, SCRIPTS_BUCKET, model_s3_key)\n",
    "print(f\"‚úÖ Model uploaded to S3: s3://{SCRIPTS_BUCKET}/{model_s3_key}\")\n",
    "\n",
    "# Save predictions to S3\n",
    "predictions_df = df_features[[\n",
    "    'customer_id', 'segment_label', 'total_spent', 'purchase_count',\n",
    "    'predicted_clv', 'recommendation'\n",
    "]]\n",
    "\n",
    "predictions_filename = '/tmp/clv_predictions.csv'\n",
    "predictions_df.to_csv(predictions_filename, index=False)\n",
    "predictions_s3_key = 'ml-predictions/clv_predictions.csv'\n",
    "s3.upload_file(predictions_filename, SCRIPTS_BUCKET, predictions_s3_key)\n",
    "print(f\"‚úÖ Predictions uploaded to S3: s3://{SCRIPTS_BUCKET}/{predictions_s3_key}\")\n",
    "\n",
    "print(\"\\nüéâ Model training and deployment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ac898",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **EDA is Critical** - Understanding data before modeling\n",
    "2. **Feature Engineering** - Creating meaningful predictive features\n",
    "3. **Random Forest** - Powerful ensemble method for regression\n",
    "4. **Model Evaluation** - R¬≤, RMSE, MAE metrics\n",
    "5. **Business Application** - Turning predictions into actions\n",
    "\n",
    "### Business Impact:\n",
    "- ‚úÖ Identified high-value customers for VIP treatment\n",
    "- ‚úÖ Predicted revenue potential for budget allocation\n",
    "- ‚úÖ Created personalized recommendations per customer\n",
    "- ‚úÖ Model saved for future predictions\n",
    "\n",
    "### Next Steps:\n",
    "1. **Monitor Model Performance** - Retrain periodically with new data\n",
    "2. **A/B Testing** - Test recommendations in production\n",
    "3. **Feature Expansion** - Add more features (product categories, seasonality)\n",
    "4. **Advanced Models** - Try XGBoost, Neural Networks\n",
    "5. **Deploy as Endpoint** - Real-time predictions via API\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
